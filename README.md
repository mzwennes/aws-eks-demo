# EKS deployment
## Summary

This repository is separated into three folders:

- `.github/workflows` - Github Action workflow
- `app` - Flask application and the Kubernetes manifests
- `terraform` - Contains the infrastructure code

**Main project characteristics**
- The app and API are served with Flask and [Gunicorn](https://gunicorn.org/)
- A container image is created and published with [Buildpacks](https://buildpacks.io)
- Github Actions is used to build, publish, and deploy the new image
- Kubernetes manifests are stored in the `app/manifests` folder
- The infrastructure (EKS, VPC, subnets, ingress controller) is deployed with Terraform

## Prerequisites
- [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)
- [awscli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)

## Application

The `app` folder contains a simple Flask application and [Gunicorn](https://gunicorn.org/) is added in front of Flask as a webserver during the build phase.

**Run it locally**

Install the dependencies (use a virtual environment):
```bash
pip3 install -r requirements.txt
```

Run the application
```bash
FLASK_APP=server.py APP_VERSION=v1.0.0 flask run
```

**Improvements**
- Both the app and API are served as a single application but they could be separated to support better isolation and scaling.

## Build

This project uses [Buildpacks](https://buildpacks.io) to build and publish the container image to Docker Hub. Buildpacks are an alternative to writing a `Dockerfile` by hand. 

Locally the [pack](https://buildpacks.io/docs/tools/pack/) command can be used to build the image:

```bash
pack build zwensman/simple-app:0.0.1 \
    --buildpack paketo-buildpacks/python \
    --builder paketobuildpacks/builder:base
```

I am not sure what to think about Buildpacks compared to regular Dockerfiles. I wanted to give it a try after seeing it multiple times during KubeCon 2022. In larger projects or organizations it can be a great tool to standardize the build process for different toolchains. However the lack of control over the vendored Buildpacks requires the use of customized Buildpacks in cases where for instance vulnerabilties are not patched.

> Initially I started out with a multi-stage Docker file which used the experimental [python3 distroless image](https://github.com/GoogleContainerTools/distroless/blob/main/experimental/python3/README.md). The final image was only 79.9MB (compared to the 303MB image which was generated by Buildpacks), however a Trivy scan detected quite a few vulnerabilities. 

## Deployment

Github Actions and Buildpacks do the heavy lifting in terms of the deployment pipeline. A Github workflow is triggered when a `release` is created or a new `tag` gets pushed to the `main` branch. This workflow handles:

1. Login to Docker Hub (environment variables are set as secrets)
2. Buildpacks builds and publishes a new container image using the Github `ref`
3. Kustomize is used to edit the image tag and version annotation in the deployment
4. These changes are pushed to the repository (to allow the use of tools such as Flux/ArgoCD)
5. AWS credentials & the EKS provider are configured
6. The updated manifests are applied to the EKS cluster

> The `APP_VERSION` environment variable gets injected in the Kubernetes deployment and is overwritten in the Kustomization file which gets updated when a new tag is pushed. This allows the app to show the version that is currently running.

**Improvements**
- Use a pull-based tool like [ArgoCD](https://argo-cd.readthedocs.io/en/stable/) or [Flux](https://fluxcd.io/) to keep the cluster in-sync with the git repository
- Add a step that scans the image for vulnerabilities in the Github Action workflow

## Infrastructure

The `terraform` folder contains a root `main.tf` file that references two modules which are located in the `modules` folder. These modules are responsible for the deployment of the VPC, subnets, security groups, roles, policies, and cluster. In addition there is a `aws-load-balancer` module which handles the deployment of the `AWS Load Balancer Controller` and an additional [IRSA](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) with proper permissions.

**Design choices**
- [AWS Load Balancer Controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller) is used as an ingress controller
- The EKS cluster is running on private subnets
- Applications can be exposed with an application load balancer (ALB, L7) by creating an `Ingress`
- Kubernetes secrets within etcd are encrypted with KMS
- Support for ipv6 within the cluster has been disabled for simplicity
- Egress traffic is handled via a NAT gateway
- Spot instances are used purely to avoid high costs

**Improvements**
- Terraform state should be stored in a proper backend
- Use a more secure AMI for the EKS node pool (such as Bottlerocket)
- Add a CICD pipeline for the Terraform (with something like [Atlantis](https://www.runatlantis.io/))
- Separate the VPC into its own module
